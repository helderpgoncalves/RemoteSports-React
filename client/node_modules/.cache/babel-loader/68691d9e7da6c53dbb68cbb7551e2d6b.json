{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { keep, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\n\nexport class SGDOptimizer extends Optimizer {\n  constructor(learningRate) {\n    super();\n    this.learningRate = learningRate;\n    this.setLearningRate(learningRate);\n  }\n\n  applyGradients(variableGradients) {\n    const varNames = Array.isArray(variableGradients) ? variableGradients.map(v => v.name) : Object.keys(variableGradients);\n    varNames.forEach((name, i) => {\n      const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n      if (gradient == null) {\n        return;\n      }\n\n      const value = ENGINE.registeredVariables[name];\n      tidy(() => {\n        const newValue = add(mul(this.c, gradient), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n  /**\n   * Sets the learning rate of the optimizer.\n   */\n\n\n  setLearningRate(learningRate) {\n    this.learningRate = learningRate;\n\n    if (this.c != null) {\n      this.c.dispose();\n    }\n\n    this.c = keep(scalar(-learningRate));\n  }\n\n  dispose() {\n    this.c.dispose();\n  }\n\n  async getWeights() {\n    return [await this.saveIterations()];\n  }\n\n  async setWeights(weightValues) {\n    weightValues = await this.extractIterations(weightValues);\n\n    if (weightValues.length !== 0) {\n      throw new Error('SGD optimizer does not have settable weights.');\n    }\n  }\n\n  getConfig() {\n    return {\n      'learningRate': this.learningRate\n    };\n  }\n  /** @nocollapse */\n\n\n  static fromConfig(cls, config) {\n    return new cls(config['learningRate']);\n  }\n\n}\n/** @nocollapse */\n\nSGDOptimizer.className = 'SGD'; // Note: Name matters for Python compatibility.\n\nregisterClass(SGDOptimizer);","map":{"version":3,"sources":["../../src/optimizers/sgd_optimizer.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,IAAR,EAAc,IAAd,QAAyB,YAAzB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,MAAR,QAAqB,eAArB;AACA,SAAoB,aAApB,QAA+E,kBAA/E;AAIA,SAAQ,SAAR,QAAwB,aAAxB;AAEA;;AACA,OAAM,MAAO,YAAP,SAA4B,SAA5B,CAAqC;AAKzC,EAAA,WAAA,CAAsB,YAAtB,EAA0C;AACxC;AADoB,SAAA,YAAA,GAAA,YAAA;AAEpB,SAAK,eAAL,CAAqB,YAArB;AACD;;AAED,EAAA,cAAc,CAAC,iBAAD,EAAgD;AAC5D,UAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,GAAlB,CAAsB,CAAC,IAAI,CAAC,CAAC,IAA7B,CADa,GAEb,MAAM,CAAC,IAAP,CAAY,iBAAZ,CAFJ;AAGA,IAAA,QAAQ,CAAC,OAAT,CAAiB,CAAC,IAAD,EAAO,CAAP,KAAY;AAC3B,YAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,CAAD,CAAjB,CAAqB,MADR,GAEb,iBAAiB,CAAC,IAAD,CAFrB;;AAGA,UAAI,QAAQ,IAAI,IAAhB,EAAsB;AACpB;AACD;;AACD,YAAM,KAAK,GAAG,MAAM,CAAC,mBAAP,CAA2B,IAA3B,CAAd;AACA,MAAA,IAAI,CAAC,MAAK;AACR,cAAM,QAAQ,GAAG,GAAG,CAAC,GAAG,CAAC,KAAK,CAAN,EAAS,QAAT,CAAJ,EAAwB,KAAxB,CAApB;AACA,QAAA,KAAK,CAAC,MAAN,CAAa,QAAb;AACD,OAHG,CAAJ;AAID,KAZD;AAaA,SAAK,mBAAL;AACD;AAED;;AAEG;;;AACH,EAAA,eAAe,CAAC,YAAD,EAAqB;AAClC,SAAK,YAAL,GAAoB,YAApB;;AACA,QAAI,KAAK,CAAL,IAAU,IAAd,EAAoB;AAClB,WAAK,CAAL,CAAO,OAAP;AACD;;AACD,SAAK,CAAL,GAAS,IAAI,CAAC,MAAM,CAAC,CAAC,YAAF,CAAP,CAAb;AACD;;AAED,EAAA,OAAO,GAAA;AACL,SAAK,CAAL,CAAO,OAAP;AACD;;AAED,QAAM,UAAN,GAAgB;AACd,WAAO,CAAC,MAAM,KAAK,cAAL,EAAP,CAAP;AACD;;AAED,QAAM,UAAN,CAAiB,YAAjB,EAA4C;AAC1C,IAAA,YAAY,GAAG,MAAM,KAAK,iBAAL,CAAuB,YAAvB,CAArB;;AACA,QAAI,YAAY,CAAC,MAAb,KAAwB,CAA5B,EAA+B;AAC7B,YAAM,IAAI,KAAJ,CAAU,+CAAV,CAAN;AACD;AACF;;AAED,EAAA,SAAS,GAAA;AACP,WAAO;AAAC,sBAAgB,KAAK;AAAtB,KAAP;AACD;AAED;;;AACA,SAAO,UAAP,CACI,GADJ,EACqC,MADrC,EACuD;AACrD,WAAO,IAAI,GAAJ,CAAQ,MAAM,CAAC,cAAD,CAAd,CAAP;AACD;;AAhEwC;AACzC;;AACO,YAAA,CAAA,SAAA,GAAY,KAAZ,C,CAAoB;;AAgE7B,aAAa,CAAC,YAAD,CAAb","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { keep, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n    constructor(learningRate) {\n        super();\n        this.learningRate = learningRate;\n        this.setLearningRate(learningRate);\n    }\n    applyGradients(variableGradients) {\n        const varNames = Array.isArray(variableGradients) ?\n            variableGradients.map(v => v.name) :\n            Object.keys(variableGradients);\n        varNames.forEach((name, i) => {\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const value = ENGINE.registeredVariables[name];\n            tidy(() => {\n                const newValue = add(mul(this.c, gradient), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    /**\n     * Sets the learning rate of the optimizer.\n     */\n    setLearningRate(learningRate) {\n        this.learningRate = learningRate;\n        if (this.c != null) {\n            this.c.dispose();\n        }\n        this.c = keep(scalar(-learningRate));\n    }\n    dispose() {\n        this.c.dispose();\n    }\n    async getWeights() {\n        return [await this.saveIterations()];\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        if (weightValues.length !== 0) {\n            throw new Error('SGD optimizer does not have settable weights.');\n        }\n    }\n    getConfig() {\n        return { 'learningRate': this.learningRate };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate']);\n    }\n}\n/** @nocollapse */\nSGDOptimizer.className = 'SGD'; // Note: Name matters for Python compatibility.\nregisterClass(SGDOptimizer);\n//# sourceMappingURL=sgd_optimizer.js.map"]},"metadata":{},"sourceType":"module"}